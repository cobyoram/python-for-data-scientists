{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Slideshow",
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3.7.2 64-bit",
      "language": "python",
      "name": "python37264bitd7afea03727a45fea46fb3907f3e48bf"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2-final"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "name": "BigData_Challenge.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cobyoram/python-for-data-scientists/blob/master/BigData_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMb0-aZVk8um",
        "colab_type": "text"
      },
      "source": [
        "# Weather's Effect on Solar Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1ekYtszk8un",
        "colab_type": "text"
      },
      "source": [
        "## 1. Introduction\n",
        "As a solar salesman and business owner, I have domain experience in the renewable energy space. While solar may seem like the best solution to energy production. However, solar can only produce so much power during so many hours of the day. The goal of this model is to try to predict solar production using the production readings of other generation methods and weather data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEsRWSy-k8uo",
        "colab_type": "text"
      },
      "source": [
        "## 2. Question\n",
        "\n",
        "### Can I predict solar generation using weather data and other generation readings for the day?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA12paQzk8uq",
        "colab_type": "text"
      },
      "source": [
        "## 3. Data\n",
        "The data comes from [kaggle](https://www.kaggle.com/nicholasjhana/energy-consumption-generation-prices-and-weather) split into two datasets. The first contains 35,064 observations. These observations include a column of instantaeously captured production levels of solar systems for each hour for four years, aggregated across Spain. The second database includes weather information at the same resolution for each of the five major cities in Spain. I intend to average the cities' weather data to represent the effective weather conditions for all of spain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnEwdwualHWM",
        "colab_type": "code",
        "outputId": "35b0708c-a02e-4928-9021-f6a6699f6f84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "source": [
        "!pip install dask_ml --quiet\n",
        "!pip install -U ipykernel --quiet\n",
        "\n",
        "import dask.dataframe as dd\n",
        "from dask_ml.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |██▋                             | 10kB 26.3MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40kB 11.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51kB 7.2MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 61kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 71kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81kB 10.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 92kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 102kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 112kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 122kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 9.1MB/s \n",
            "\u001b[?25h\u001b[?25l\r\u001b[K     |▌                               | 10kB 26.3MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 30.6MB/s eta 0:00:01\r\u001b[K     |█▋                              | 30kB 34.8MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 37.5MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51kB 38.7MB/s eta 0:00:01\r\u001b[K     |███▏                            | 61kB 40.9MB/s eta 0:00:01\r\u001b[K     |███▋                            | 71kB 41.9MB/s eta 0:00:01\r\u001b[K     |████▏                           | 81kB 42.2MB/s eta 0:00:01\r\u001b[K     |████▊                           | 92kB 43.4MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 102kB 44.6MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 112kB 44.6MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 122kB 44.6MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 133kB 44.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 143kB 44.6MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 153kB 44.6MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 163kB 44.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 174kB 44.6MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 184kB 44.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 194kB 44.6MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 204kB 44.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 215kB 44.6MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 225kB 44.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 235kB 44.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 245kB 44.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 256kB 44.6MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 266kB 44.6MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 276kB 44.6MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 286kB 44.6MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 296kB 44.6MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 307kB 44.6MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 317kB 44.6MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 327kB 44.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 337kB 44.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 348kB 44.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 358kB 44.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 368kB 44.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 378kB 44.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 389kB 44.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 399kB 44.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 409kB 44.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 419kB 44.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 430kB 44.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 440kB 44.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 450kB 44.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 460kB 44.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 471kB 44.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 481kB 44.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 491kB 44.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 501kB 44.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 512kB 44.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 522kB 44.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 532kB 44.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 542kB 44.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 552kB 44.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 563kB 44.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 573kB 44.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 583kB 44.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 593kB 44.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 604kB 44.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 614kB 44.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 624kB 44.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 634kB 44.6MB/s \n",
            "\u001b[?25h\u001b[?25l\r\u001b[K     |▋                               | 10kB 26.7MB/s eta 0:00:01\r\u001b[K     |█▎                              | 20kB 35.1MB/s eta 0:00:01\r\u001b[K     |██                              | 30kB 42.2MB/s eta 0:00:01\r\u001b[K     |██▋                             | 40kB 46.3MB/s eta 0:00:01\r\u001b[K     |███▎                            | 51kB 47.9MB/s eta 0:00:01\r\u001b[K     |████                            | 61kB 50.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 71kB 52.2MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 81kB 53.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 92kB 54.1MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 102kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 112kB 54.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 122kB 54.9MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 133kB 54.9MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 143kB 54.9MB/s eta 0:00:01\r\u001b[K     |██████████                      | 153kB 54.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 163kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 174kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 184kB 54.9MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 194kB 54.9MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 204kB 54.9MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 215kB 54.9MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 225kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 235kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 245kB 54.9MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 256kB 54.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 266kB 54.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 276kB 54.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 286kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 296kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 307kB 54.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 317kB 54.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 327kB 54.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 337kB 54.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 348kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 358kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 368kB 54.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 378kB 54.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 389kB 54.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 399kB 54.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 409kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 419kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 430kB 54.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 440kB 54.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 450kB 54.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 460kB 54.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 471kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 481kB 54.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 491kB 54.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 501kB 54.9MB/s \n",
            "\u001b[?25h  Building wheel for tornado (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for locket (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 122kB 8.8MB/s \n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipykernel~=4.10, but you'll have ipykernel 5.2.1 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ContextualVersionConflict",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mContextualVersionConflict\u001b[0m                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f277ff171b1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdask_ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/dask_ml/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mDistributionNotFound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# package is not installed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_distribution\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_provider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected string, Requirement, or Distribution\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_provider\u001b[0;34m(moduleOrReq)\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;34m\"\"\"Return an IResourceProvider for the named module or requirement\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mworking_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mrequire\u001b[0;34m(self, *requirements)\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0mincluded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meven\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mwere\u001b[0m \u001b[0malready\u001b[0m \u001b[0mactivated\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mworking\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m         \"\"\"\n\u001b[0;32m--> 901\u001b[0;31m         \u001b[0mneeded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneeded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(self, requirements, env, installer, replace_conflicting, extras)\u001b[0m\n\u001b[1;32m    790\u001b[0m                 \u001b[0;31m# Oops, the \"best\" so far conflicts with a dependency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m                 \u001b[0mdependent_req\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequired_by\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mVersionConflict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdependent_req\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m             \u001b[0;31m# push the new requirements onto the stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mContextualVersionConflict\u001b[0m: (distributed 1.25.3 (/usr/local/lib/python3.6/dist-packages), Requirement.parse('distributed>=2.4.0'), {'dask-ml'})"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAzjcIGAk8ur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import seaborn as sns\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fLN3O_6k8uu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    # For local environment, load in csvs from directory\n",
        "    raw_energy = dd.read_csv('energy_dataset.csv')\n",
        "    raw_weather = dd.read_csv('weather_features.csv')\n",
        "except FileNotFoundError as e:\n",
        "    # Load the dataframes from their location online if that doesn't work\n",
        "    print('Attempting to read datasets from online csv files on Github')\n",
        "    raw_energy = dd.read_csv('https://raw.githubusercontent.com/cobyoram/Experimental_Design__Thinkful_capstone_1/master/energy_dataset.csv')\n",
        "    raw_weather = dd.read_csv('https://raw.githubusercontent.com/cobyoram/Experimental_Design__Thinkful_capstone_1/master/weather_features.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDLOoi3Xk8ux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Familiarize with energy data\n",
        "print(raw_energy.info())\n",
        "raw_energy.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK5RRCbpk8u0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Familiarize with weather data\n",
        "print(raw_weather.info())\n",
        "raw_weather.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDYyaRkbk8u3",
        "colab_type": "text"
      },
      "source": [
        "## Clean the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm1hbWCGk8u4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We want to be able to combine these datasets together. So we need to aggregate the weather data across spain, and set a shared key variable using the date and time\n",
        "# First remove the +01:00 and like-formatted timezone aware attributes from datetime string in each dataframe\n",
        "raw_weather['dt_iso'] = raw_weather['dt_iso'].str.replace('\\+[0-9][0-9]:[0-9][0-9]', '', regex=True)\n",
        "raw_energy['time'] = raw_energy['time'].str.replace('\\+[0-9][0-9]:[0-9][0-9]', '', regex=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeC3vQ40Slab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_weather.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_n6ML4sSaY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a new meta to map partitions to: datetime object type\n",
        "meta = ('dt_iso', 'datetime64[ns]')\n",
        "\n",
        "# Map the values to the new datetime objects\n",
        "raw_weather['date'] = raw_weather['dt_iso'].map_partitions(pd.to_datetime, meta=meta)\n",
        "\n",
        "# Drop old datetime string columns\n",
        "raw_weather = raw_weather.drop('dt_iso', axis=1)\n",
        "raw_weather.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ1O7N4GUg3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ensure the transformation worked\n",
        "type(raw_weather)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2LzwnY0TS8V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Repeat for solar dataset\n",
        "meta = ('time', 'datetime64[ns]')\n",
        "raw_energy['date'] = raw_energy['time'].map_partitions(pd.to_datetime, meta=meta)\n",
        "raw_energy = raw_energy.drop('time', axis=1)\n",
        "raw_energy.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4xoAtTNWhQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_energy.date.dtype"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCNlpOa8k8vB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We group the weather data by the date, aggregating by mean for numerical data, and mode for categorical data\n",
        "agg_weather_num = raw_weather[['date'] + list(raw_weather.select_dtypes('number').columns)].groupby(by='date').mean()\n",
        "agg_weather_cat = raw_weather[['date'] + list(raw_weather.select_dtypes('object').columns)].groupby(by='date').max()\n",
        "# We concat the two aggregated weather datasets together to form one\n",
        "agg_weather = agg_weather_cat.join(agg_weather_num, on='date').reset_index()\n",
        "agg_weather.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJa9C26yW8Qx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_energy.date.isna().sum().compute()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2ygLJJlbVLP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agg_weather['date'].count().compute()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSqsnFUJba3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "raw_energy['date'].count().compute()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c7jJCIqci1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "first_date = agg_weather.loc[0, 'date'].compute()[0]\n",
        "\n",
        "# agg_weather['joincol'] = "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLJW-sG8eEEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "first_date"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5B1Za2Phkmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "first_date_e = raw_energy.loc[0, 'date'].compute()[0]\n",
        "first_date_e"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uTTRWc9k8vD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we join the weather and energy datasets together on the date column\n",
        "\n",
        "energy_weather_data = agg_weather.merge(raw_energy, on='date')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgl4b4NfY4E0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "energy_weather_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Irhmm_CoDl6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def missingness_summary(df, **kwargs):\n",
        "    '''\n",
        "    This function creates a series representing what percentage of data is null for each column of a dataframe\n",
        "\n",
        "    You can use a number of kwargs to customize the function. Those include:\n",
        "    kwargs:\n",
        "        print_log   -   [True, False]: If true, will print the output before returning the Series (default False)\n",
        "        sort        -   ['asc', 'desc']: Allows you to sort the data by ascending or descending (default 'desc')\n",
        "    \n",
        "    Returns Series with index = column names and value = percentage of nulls in column\n",
        "\n",
        "    '''\n",
        "    s = df.isna().sum()*100/len(df)\n",
        "\n",
        "    sort = 'desc'\n",
        "    if kwargs.get('sort'):\n",
        "        sort = kwargs.get('sort')\n",
        "    if sort == 'asc':\n",
        "        s.sort_values(ascending=True, inplace=True)\n",
        "    elif sort == 'desc':\n",
        "        s.sort_values(ascending=False, inplace=True)\n",
        "\n",
        "    print_log = False\n",
        "    if kwargs.get('print_log'):\n",
        "        print_log = kwargs.get('print_log')\n",
        "    if print_log == True:\n",
        "        print(s)\n",
        "\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kNiYRrrk8vJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Next we get rid of columns that are missing too much data\n",
        "missings = energy_weather_data.isna().sum()/len(energy_weather_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk2xtFCWohsi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "missings.compute()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ekq3ECQ8p7Xk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drop_cols = [col for col in missings.loc[missings == 1].index]\n",
        "ew_clean = energy_weather_data.drop(drop_cols, axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSebu7Ae1AF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drop_cols"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CNxRFzCsC-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ew_clean.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2hvbvyiz76d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def myinterp (xs, secs=[], vals=[]):\n",
        "    ret = np.interp(xs, secs, vals)\n",
        "    return ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF6Z-nwksDWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# There is not much missing data now, so we'll drop the missing values to have a non-null dataset\n",
        "ew_clean_nn = ew_clean.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VnstJH_6Zn-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def repeats_summary(df, sort='desc', print_log=False, value_agg='none', value=0):\n",
        "    repeats_percents = []\n",
        "    print_value = []\n",
        "    for col in df.columns:\n",
        "        if value_agg == 'none':\n",
        "            value = value\n",
        "            print_value = []\n",
        "        elif value_agg == 'mode':\n",
        "            value = df[col].mode().iloc[0]\n",
        "        elif value_agg == 'mean':\n",
        "            value = df[col].mean().iloc[0]\n",
        "        elif value_agg == 'median':\n",
        "            value = df[col].median().iloc[0]\n",
        "        elif value_agg == 'max':\n",
        "            value = df[col].max().iloc[0]\n",
        "        elif value_agg == 'min':\n",
        "            value = df[col].min().iloc[0]\n",
        "        else: raise ValueError('Wrong entry for \\'value_agg\\'. Will accept \\'mode\\', \\'mean\\', \\'median\\', \\'max\\', \\'min\\'')\n",
        "\n",
        "        repeats_percents.append(len(df.loc[df[col] == value])*100/len(df))\n",
        "        print_value.append(value)\n",
        "\n",
        "    S = pd.Series(repeats_percents, index=df.columns)\n",
        "    if sort == 'desc':\n",
        "        S = S.sort_values(ascending=False)\n",
        "    elif sort == 'asc':\n",
        "        S = S.sort_values(ascending=True)\n",
        "    elif sort == 'none':\n",
        "      None\n",
        "    else: raise ValueError('Wrong entry for \\'sort\\'. Will accept \\'asc\\' or \\'desc\\'')\n",
        "\n",
        "    if print_log:\n",
        "        print(f'Repeated values: {print_value}\\n{S}')\n",
        "\n",
        "    return S\n",
        "\n",
        "def drop_null_rows(df):\n",
        "    for col in df.columns:\n",
        "        df.drop(df.loc[df[col].isnull()].index, axis=0, inplace=True)\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUKxfRAlk8vN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We drop columns that repeat 0 a ridiculous amount of times over the 3 years.\n",
        "repeats = repeats_summary(ew_clean_nn, sort='none', print_log=True, value=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rr3QsmyoeVzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ew_clean_nn = ew_clean_nn.drop(repeats.loc[repeats > 90].index, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab_0zWcDk8vQ",
        "colab_type": "text"
      },
      "source": [
        "## Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-ZAJTtSk8vR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print out the unique classes in each categorical variable\n",
        "for col in ew_clean_nn.select_dtypes('object').columns:\n",
        "    print(col)\n",
        "    print(ew_clean[col].unique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w34i4XWFk8vT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop categorical variables that we won't need or that have too many unique clases\n",
        "cat_drop_cols = ['city_name', 'weather_description', 'weather_icon']\n",
        "ew_feats = ew_clean_nn.drop(cat_drop_cols, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcYyPXlMQHB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dum_cols = ew_feats.select_dtypes('object').columns\n",
        "dum_cols"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWi8HKwKQ9XA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ew_feats.categorize(ew_feats[dum_cols]).head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JpnlfL-k8vV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create dummy variables for categorical columns\n",
        "\n",
        "dums = dd.get_dummies(ew_feats.categorize(ew_feats[dum_cols]), drop_first=True)\n",
        "dums"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv7NqHlaSbe6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dums.head()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRGHrzpok8vX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check for multicollinearity\n",
        "corr = dums.corr()\n",
        "plt.figure(figsize=(17,15))\n",
        "sns.heatmap(corr)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfHjyYqhk8va",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop collinear variables after interpreting the heatmap\n",
        "manual_drop_cols = [col for col in dums.columns if 'forecast' in col or 'price' in col or 'total' in col]\n",
        "manual_drop_cols += ['temp_min', 'temp_max']\n",
        "manual_drop_cols"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaxobPJOk8vd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dums = dums.drop(manual_drop_cols, axis=1)\n",
        "dums"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwosOtyMk8vf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Explore the behavior of generation columns to help us choose a model\n",
        "generation_cols = [col for col in dums.columns if 'generation' in col]\n",
        "plot_df = dums[generation_cols + ['date']]\n",
        "melted_df = plot_df.melt(id_vars='date')\n",
        "plt.figure(figsize=(15,7))\n",
        "sns.lineplot(x=melted_df['date'].dt.hour, y=melted_df['value'], hue=melted_df['variable'])\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnQmpM80W38V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "def make_subplots(df, plotfunc=None, func_args=[], func_kwargs={}, limitx=8, each_size=3, **kwargs):\n",
        "    '''\n",
        "    Makes a subplot, filled with a given plotting function\n",
        "    '''\n",
        "    columns = df.columns\n",
        "    len_cols = len(columns)\n",
        "\n",
        "    try_num = len_cols\n",
        "    while True:\n",
        "        sq = math.sqrt(try_num)\n",
        "        if sq == int(sq):\n",
        "            break\n",
        "        try_num += 1\n",
        "    count_dimensions = tuple([sq, sq])\n",
        "\n",
        "    if count_dimensions[0] > limitx:\n",
        "        count_dimensions = tuple([int(len_cols/limitx) + 1, limitx])\n",
        "\n",
        "    dimensions = tuple([count_dimensions[1] * each_size, count_dimensions[0] * each_size])\n",
        "    plt.figure(figsize=dimensions)\n",
        "\n",
        "    for i, col in enumerate(columns, 1):\n",
        "        plt.subplot(count_dimensions[0], count_dimensions[1], i)\n",
        "        plotfunc(df, col, *func_args, **func_kwargs)\n",
        "        plt.title(col)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBW8m495k8vh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Explore the distributions of the weather variables\n",
        "weather_main_cols = [col for col in dums.columns if 'weather_main' in col]\n",
        "dist_df = dums.drop(generation_cols + weather_main_cols + ['date'], axis=1)\n",
        "\n",
        "def plot_dist(df, col):\n",
        "    sns.distplot(dist_df[col])\n",
        "\n",
        "make_subplots(dist_df, plot_dist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O86uJocik8vj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dums['hour'] = dums['date'].dt.hour\n",
        "dums['month'] = dums['date'].dt.month\n",
        "dums['year'] = dums['date'].dt.year"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfkDPwCsk8vl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import joblib\n",
        "from dask_ml.model_selection import train_test_split\n",
        "\n",
        "# Spleat features and target into train and test splits\n",
        "X = dums.drop(['generation solar', 'date'], axis = 1)\n",
        "Y = dums['generation solar']\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = .1)\n",
        "\n",
        "X_train.persist()\n",
        "X_test.persist()\n",
        "Y_train.persist()\n",
        "Y_test.persist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGWNyHw2k8vq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from dask.distributed import Client\n",
        "client = Client()\n",
        "\n",
        "# Test the Random Forest Regressor model\n",
        "rfr = RandomForestRegressor(max_depth=8)\n",
        "\n",
        "with joblib.parallel_backend('dask'):\n",
        "  rfr.fit(X_train, Y_train)\n",
        "\n",
        "print(rfr.score(X_train, Y_train))\n",
        "print(rfr.score(X_test, Y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1D7ru52k8vt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Test the Gradient Boosting Regressor model\n",
        "gbr = GradientBoostingRegressor()\n",
        "\n",
        "with joblib.parallel_backend('dask'):\n",
        "  gbr.fit(X_train, Y_train)\n",
        "  \n",
        "print(gbr.score(X_train, Y_train))\n",
        "print(gbr.score(X_test, Y_test))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etIJOTGYk8vw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.Series(gbr.feature_importances_, index=X_train.columns).sort_values(ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RJeB7NOk8vy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Test a linear regression model\n",
        "lrm = LinearRegression()\n",
        "\n",
        "with joblib.parallel_backend('dask'):\n",
        "  lrm.fit(X_train, Y_train)\n",
        "  \n",
        "print(lrm.score(X_train, Y_train))\n",
        "print(lrm.score(X_test, Y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94wKRu_8rQTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from dask_ml.model_selection import GridSearchCV as DaskGS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRI4gq37k8v1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gs = DaskGS(\n",
        "    gbr,\n",
        "    param_grid = {'n_estimators': [10, 100, 500],\n",
        "                        'max_features': [2,4,8],\n",
        "                        # 'learning_rate': [.05,.1,.2,.4,.8,1],\n",
        "                        'max_depth': [2,4,8]},\n",
        "    cv=5)\n",
        "\n",
        "gs.fit(X_train, Y_train)\n",
        "\n",
        "\n",
        "print(gs.best_params_)\n",
        "print(gs.best_score_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRUyjU4Gk8v3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implement the Random Forest Regressor with optimized hyperparameters\n",
        "gbr_best = GradientBoostingRegressor(\n",
        "    max_depth=gs.best_params_['max_depth'],\n",
        "    n_estimators=gs.best_params_['n_estimators'],\n",
        "    max_features=gs.best_params_['max_features'])\n",
        "\n",
        "with joblib.paralell_backend('dask'):\n",
        "  gbr_best.fit(X_train, Y_train)\n",
        "\n",
        "print(gbr_best.score(X_train, Y_train))\n",
        "print(gbr_best.score(X_test, Y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qx5dxFSk8v4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.Series(gbr.feature_importances_, index=X_train.columns).sort_values(ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpiEDJ3wk8v7",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate\n",
        "### Print metrics for the test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrlwN4qzk8v8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from statsmodels.tools.eval_measures import mse, rmse\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "preds = gbr_best.predict(X_test)\n",
        "\n",
        "ds.print_evaluation_metrics(Y_test, preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1SGgLyMk8v-",
        "colab_type": "text"
      },
      "source": [
        "## 5. Results\n",
        "While the model could use some improvement, given the time constraint on this project, a score of .84 was a satisfactory enough R^2 to support the idea that yes, you can predict solar generation at an hour given other generations and weather. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X65LDZg9k8wA",
        "colab_type": "text"
      },
      "source": [
        "## 6. Discussion and Recommendation\n",
        "Solar is still a pretty hot topic for homeowners and energy production companies around the globe. While it is important to remember that without seemingly impossible storage systems and planning, it is not a viable baseload solution. Knowing how much production to expect at any time of the day is vital to keeping the energy demand met and preventing transmission overloads.\n",
        "\n",
        "If I were to take this project further, I would use the time-series attributes of my data to create a forecast model that could predict future generation instead of current generation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppj9X68DBwD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}